{"backend_state":"running","connection_file":"/projects/c7a1100d-10f9-4de1-b172-d38d7d09565f/.local/share/jupyter/runtime/kernel-244d2faa-8662-4c24-a991-1dcd8764692d.json","kernel":"python3-ubuntu","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1698291402094,"id":"918358","input":"import pandas as pd\nimport seaborn as sns\n\nfrom sklearn.manifold import TSNE\n\n# Specify the perplexity and learning rate values\nperplexity_value = 50  # You can adjust this value\nlearning_rate_value = 30  # You can adjust this value\n\n# Initialize the t-SNE model with specified perplexity and learning rate\ntsne = TSNE(n_components=2, perplexity=perplexity_value, learning_rate=learning_rate_value, random_state=42)\n\n# Fit the t-SNE model to your data\nembeddings_2d = tsne.fit_transform(embedd_array)\n\n# Create binary labels as a list (0 for 'requirement', 1 for 'standard')\nlabels = df.label.map({'standard': 1, 'requirement': 0}).tolist()\n\n# Define colors and markers for the two classes\ncolors = sns.color_palette('Set1', n_colors=2)  # Custom color palette\nmarkers = 'o'  # Circle markers for both classes\n\n# Convert embeddings_2d array into a DataFrame with index values\ndf = pd.DataFrame(embeddings_2d, columns=['Dimension 1', 'Dimension 2'])\ndf['label'] = labels\n\n# Create a scatter plot of the t-SNE embeddings for both classes\nplt.figure(figsize=(10, 8))\nsns.set(style='whitegrid')  # Set Seaborn style with gridlines\n\nfor label in set(labels):\n    df_label = df[df['label'] == label]\n    sns.scatterplot(data=df_label, x='Dimension 1', y='Dimension 2', hue='label', palette=[colors[int(label)]], marker=markers, edgecolor='k', s=100)\n\n# Create legends for the classes\nclass_labels = ['requirement', 'standard']  # Map the labels back to their original names\nlegend_handles = [plt.Line2D([0], [0], marker=markers, color='w', label=class_labels[int(label)], markersize=10,\n                              markerfacecolor=colors[int(label)]) for label in set(labels)]\nplt.legend(handles=legend_handles, title='Classes')\n\nplt.title(\"t-SNE Visualization of Embeddings with Class Labels\", fontsize=16)\nplt.xlabel(\"t-SNE Dimension 1\", fontsize=12)\nplt.ylabel(\"t-SNE Dimension 2\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)  # Turn off gridlines\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Save the plot as a PNG image\nplt.savefig('tsne_visualization_basetraining.png', dpi=300, bbox_inches='tight')\n\nplt.show()\n","kernel":"python3-ubuntu","pos":20,"start":1698291401948,"state":"done","type":"cell"}
{"cell_type":"code","end":1698295359270,"exec_count":6,"id":"ef3225","input":"def requirement_to_embedding(model, tokenizer, requirement):\n    input = tokenizer(requirement, padding=True, truncation=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        input = input.to(\"cpu\")  # Copy input to CPU\n        outputs = model(**input)  # Run model without labels to get outputs\n        embeddings = outputs.hidden_state\n        embeddings = embeddings.detach().numpy()\n    return embeddings","kernel":"python3-ubuntu","pos":4,"start":1698295359268,"state":"done","type":"cell"}
{"cell_type":"code","end":1698295360494,"exec_count":7,"id":"996669","input":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimport numpy as np\nembeddings = [ requirement_to_embedding ( model , tokenizer , requirement ) for\nrequirement in requirements ]\nembedd_array = np . stack ( embeddings )\nembedd_array . shape\n","kernel":"python3-ubuntu","output":{"0":{"ename":"AttributeError","evalue":"'SequenceClassifierOutput' object has no attribute 'hidden_state'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [ requirement_to_embedding ( model , tokenizer , requirement ) \u001b[38;5;28;01mfor\u001b[39;00m\n\u001b[1;32m      6\u001b[0m requirement \u001b[38;5;129;01min\u001b[39;00m requirements ]\n\u001b[1;32m      7\u001b[0m embedd_array \u001b[38;5;241m=\u001b[39m np \u001b[38;5;241m.\u001b[39m stack ( embeddings )\n\u001b[1;32m      8\u001b[0m embedd_array \u001b[38;5;241m.\u001b[39m shape\n","Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [ \u001b[43mrequirement_to_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m\n\u001b[1;32m      6\u001b[0m requirement \u001b[38;5;129;01min\u001b[39;00m requirements ]\n\u001b[1;32m      7\u001b[0m embedd_array \u001b[38;5;241m=\u001b[39m np \u001b[38;5;241m.\u001b[39m stack ( embeddings )\n\u001b[1;32m      8\u001b[0m embedd_array \u001b[38;5;241m.\u001b[39m shape\n","Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mrequirement_to_embedding\u001b[0;34m(model, tokenizer, requirement)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Copy input to CPU\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)  \u001b[38;5;66;03m# Run model without labels to get outputs\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_state\u001b[49m\n\u001b[1;32m      7\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n","\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'hidden_state'"]}},"pos":5,"start":1698295360068,"state":"done","type":"cell"}
{"cell_type":"code","end":1698296137582,"exec_count":2,"id":"eae6e3","input":"\nfrom sklearn.manifold import TSNE\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizerFast\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\n\n","kernel":"python3-ubuntu","output":{"0":{"name":"stderr","text":"/projects/c7a1100d-10f9-4de1-b172-d38d7d09565f/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n  from pandas.core import (\n"}},"pos":0,"start":1698296135611,"state":"done","type":"cell"}
{"cell_type":"code","end":1698296138479,"exec_count":3,"id":"8c7583","input":"\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\n\n# Define the directory where you saved the model and tokenizer\noutput_dir = './model_save/'\n\n# Load the tokenizer\n#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# Load the model\n#model = BertForSequenceClassification.from_pretrained('bert-base-uncased', output_hidden_states=True)\n\n","kernel":"python3-ubuntu","pos":1,"start":1698296138477,"state":"done","type":"cell"}
{"cell_type":"code","end":1698296140800,"exec_count":4,"id":"1e664e","input":"df = pd.read_csv(\"embed_data\")\ndf = df.loc[(df.label == \"standard\") | (df.label == \"requirement\")]\nprint(df.groupby(\"document\").count())\nprint(df.head())","kernel":"python3-ubuntu","output":{"0":{"name":"stdout","text":"                text  label\ndocument                   \nAASHTO            27     27\nASABE             39     39\nATSM              99     99\nIEEE              13     13\nISO               17     17\nJCanadaWelding   350    350\nJpierburg        214    214\nJtoho            159    159\nMilSpec            9      9\nSKADish          289    289\nSKAMid           291    291\n                                                text        label  \\\n0  CSP_Mid.CBF shall have a Maintenance Down Time...  requirement   \n1  When commanded, CSP_Mid.CBF shall perform auto...  requirement   \n2   Each box end end blast station paddle lift sh...  requirement   \n3   Each pipe shall be transferred into the box e...  requirement   \n4   Each of the vrollers shall be used on many st...  requirement   \n\n         document  \n0          SKAMid  \n1          SKAMid  \n2  JCanadaWelding  \n3  JCanadaWelding  \n4  JCanadaWelding  \n"}},"pos":2,"start":1698296140789,"state":"done","type":"cell"}
{"cell_type":"code","end":1698296146350,"exec_count":5,"id":"550e86","input":"requirements = df.text.tolist()\nlabels = df.label.tolist()\ndocuments = df.document.tolist()\n#print(documents)","kernel":"python3-ubuntu","pos":3,"start":1698296146348,"state":"done","type":"cell"}
{"cell_type":"code","end":1698297050627,"exec_count":12,"id":"7303b1","input":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n# Specify the perplexity and learning rate values\nperplexity_value = 6  # You can adjust this value\nlearning_rate_value = 10  # You can adjust this value\n\n# Initialize the t-SNE model with specified perplexity and learning rate\ntsne = TSNE(n_components=2, perplexity=perplexity_value, learning_rate=learning_rate_value, random_state=42)\n\n# Fit the t-SNE model to your data\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# Create a DataFrame with the t-SNE embeddings and labels\ndf = pd.DataFrame(embeddings_2d, columns=['Dimension 1', 'Dimension 2'])\ndf['label'] = labels\ndf['document_class'] = df['document']  # Create a new column for document classes\n\n# Create a dictionary to map document classes to unique colors\nunique_document_classes = df['document_class'].unique()\ncolors = sns.color_palette('husl', n_colors=len(unique_document_classes))\ndocument_class_color_mapping = {doc_class: color for doc_class, color in zip(unique_document_classes, colors)}\n\n# Assign a color to each data point based on its document class\ndf['color'] = df['document_class'].map(document_class_color_mapping)\n\n# Create a scatter plot of the t-SNE embeddings for different document classes\nplt.figure(figsize=(10, 8))\nsns.set(style='whitegrid')  # Set Seaborn style with gridlines\n\nfor doc_class, color in document_class_color_mapping.items():\n    df_class = df[df['document_class'] == doc_class]\n    marker = 's' if doc_class == 'Triangle' else 'o'\n    sns.scatterplot(data=df_class, x='Dimension 1', y='Dimension 2', hue='document_class', palette=[color], marker=marker, edgecolor='k', s=100)\n    \n\n\n# Create legends for the document classes\nlegend_handles = [plt.Line2D([0], [0], marker=marker, color='w', label=doc_class, markersize=10,\n                              markerfacecolor=color) for doc_class, color in document_class_color_mapping.items()]\nplt.legend(handles=legend_handles, title='Document Classes')\n\nplt.title(\"t-SNE Visualization of Embeddings with Document Classes and IDs\", fontsize=16)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)  # Turn off gridlines\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Save the plot as a PNG image\nplt.savefig('tsne_visualization_with_classes.png', dpi=300, bbox_inches='tight')\n\nplt.show()\n","kernel":"python3-ubuntu","output":{"0":{"ename":"AttributeError","evalue":"'list' object has no attribute 'shape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39mperplexity_value, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate_value, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Fit the t-SNE model to your data\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m embeddings_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with the t-SNE embeddings and labels\u001b[39;00m\n\u001b[1;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(embeddings_2d, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDimension 1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDimension 2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py:1118\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m-> 1118\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py:828\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    829\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"]}},"pos":30,"start":1698297050589,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"10155a","input":"import pandas as pd\n\n# Create a DataFrame to store the points and their corresponding text values\ndf = pd.DataFrame({'Point': range(len(cluster_labels)), 'Cluster Label': cluster_labels})\ndf['Text'] = df['Point'].map(text_dict)  # Assuming text_dict is a dictionary mapping points to their text values\nprint(df)\nNote: This assumes you have a dictionary called `text_dict` that maps each point to its corresponding text value. Adjust the code according to your specific setup.","pos":27,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"43d5dc","input":"from sklearn.metrics.pairwise import cosine_similarity\n\n# Calculate cosine similarities between embeddings\ncosine_distances = 1 - cosine_similarity(embedd_array)\n","pos":17,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"499fcd","input":"","pos":33,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7791c0","input":"import matplotlib.pyplot as plt\nimport numpy as np\nimport datetime  # Import the datetime module\n\n# Create a list of class pairs and their corresponding average distances\nclass_pairs = list(inter_class_distances.keys())\navg_distances = list(inter_class_distances.values())\n\n# Create a matrix of the average distances\nn_classes = len(unique_classes)\ndist_matrix = np.zeros((n_classes, n_classes))\nfor i in range(len(class_pairs)):\n    class1, class2 = class_pairs[i]\n    index1 = unique_classes.index(class1)\n    index2 = unique_classes.index(class2)\n    dist_matrix[index1, index2] = avg_distances[i]\n    dist_matrix[index2, index1] = avg_distances[i]  # Set the symmetric value\n\n# Plot the heatmap\nplt.figure(figsize=(10, 8))\nheatmap = plt.imshow(dist_matrix, cmap='coolwarm', interpolation='none')\nplt.xticks(range(n_classes), unique_classes, rotation=90)\nplt.yticks(range(n_classes), unique_classes)\nplt.title(\"Average Cosine Distance between Classes\")\nplt.colorbar(label=\"Distance\")\n\n# Annotate the boxes with cosine similarity values rounded to two decimal places\nfor i in range(n_classes):\n    for j in range(n_classes):\n        plt.text(j, i, f'{dist_matrix[i, j]:.2f}', ha='center', va='center', color='white', fontsize=10)\n\n# Get the current date and time\ncurrent_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n# Define the filename with the current date and time\nfilename = f\"Avg_Cosine_Dist_bertbase_{current_datetime}.png\"\n\n# Save the figure\nplt.savefig(filename)\n\nplt.show()\n","pos":16,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f6b9c3","input":"import matplotlib.pyplot as plt\n\n# Create a heatmap of the cosine distances\nplt.figure(figsize=(10, 8))\nplt.imshow(cosine_distances, cmap='seismic', interpolation='none')\nplt.colorbar(label='Cosine Distance')\nplt.title('Cosine Distance Heatmap')\nplt.xlabel('Requirements')\nplt.ylabel('Requirements')\nplt.show()\n","pos":18,"state":"done","type":"cell"}
{"cell_type":"code","id":"9bf7d8","input":"# Import additional libraries\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\n# Perform PCA on the embeddings\npca = PCA(n_components=2)\nembeddings_pca = pca.fit_transform(embeddings_2d)\n\n# Create a scatter plot of the PCA embeddings for both classes\nplt.figure(figsize=(10, 8))\nsns.set(style='whitegrid')  # Set Seaborn style with gridlines\n\nfor label in set(labels):\n    df_label = df[df['label'] == label]\n    sns.scatterplot(data=df_label, x='Dimension 1', y='Dimension 2', hue='label', palette=[colors[int(label)]], marker=markers, edgecolor='k', s=100)\n\nplt.title(\"PCA Visualization of Embeddings with Class Labels\", fontsize=16)\nplt.xlabel(\"PCA Dimension 1\", fontsize=12)\nplt.ylabel(\"PCA Dimension 2\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)  # Turn off gridlines\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Save the plot as a PNG image\nplt.savefig('pca_visualization_basetraining.png', dpi=300, bbox_inches='tight')\n\nplt.show()\n","pos":28,"state":"done","type":"cell"}
{"cell_type":"code","id":"a27b5c","input":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n# Assuming embedd_array is a numpy array of embeddings\n# And document is a list of labels corresponding to each embedding\n\n# Create an empty dictionary to store embeddings for each class\nclass_embeddings = {}\n\n# Group embeddings by class\nfor i, label in enumerate(documents):\n    if label not in class_embeddings:\n        class_embeddings[label] = []\n    class_embeddings[label].append(normalized_embeddings[i])\n\n# Calculate the average cosine distance for each class\nclass_avg_distances = {}\n\nfor label, embeddings in class_embeddings.items():\n    # Calculate cosine distances within each class\n    distances = 1 - cosine_similarity(embeddings)\n    # Calculate the average distance for this class\n    avg_distance = np.mean(distances)\n    class_avg_distances[label] = avg_distance\n\n# Now, class_avg_distances contains the average cosine distance for each class\nfor label, avg_distance in class_avg_distances.items():\n    print(f\"Class {label}: Average Cosine Distance = {avg_distance}\")\n","pos":9,"state":"done","type":"cell"}
{"cell_type":"code","id":"a8a8f8","input":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Assuming embedd_array is a numpy array of embeddings\n# And documents is a list of labels corresponding to each embedding\n\n# Create an empty dictionary to store embeddings for each class\nclass_embeddings = {}\n\n# Group embeddings by class\nfor i, label in enumerate(documents):\n    if label not in class_embeddings:\n        class_embeddings[label] = []\n    class_embeddings[label].append(normalized_embeddings[i])\n\n# Calculate inter-class cosine distances\ninter_class_distances = {}\n\n# Iterate through unique pairs of classes\nunique_classes = list(class_embeddings.keys())\nfor i in range(len(unique_classes)):\n    for j in range(i + 1, len(unique_classes)):\n        class1 = unique_classes[i]\n        class2 = unique_classes[j]\n        \n        # Calculate cosine similarity between embeddings of class1 and class2\n        distances = 1 -cosine_similarity(class_embeddings[class1], class_embeddings[class2])\n        \n        # Calculate the average distance for this pair of classes\n        avg_distance = np.mean(distances)\n        \n        # Store the average distance in the dictionary\n        inter_class_distances[(class1, class2)] = avg_distance\n\n# Now, inter_class_distances contains the average cosine distance between each pair of classes\nfor (class1, class2), avg_distance in inter_class_distances.items():\n    print(f\"Average Cosine Distance between Class {class1} and Class {class2}: {avg_distance}\")\n","pos":12,"state":"done","type":"cell"}
{"cell_type":"code","id":"e09da0","input":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist\n\n# Assuming embedd_array is a numpy array of embeddings\n# And documents is a list of labels corresponding to each embedding\n\n# Create an empty dictionary to store embeddings for each class\nclass_embeddings = {}\n\n# Group embeddings by class\nfor i, label in enumerate(documents):\n    if label not in class_embeddings:\n        class_embeddings[label] = []\n    class_embeddings[label].append(normalized_embeddings[i])\n\n# Calculate inter-class cosine distances\ninter_class_distances = {}\n\n# Iterate through unique pairs of classes\nunique_classes = list(class_embeddings.keys())\nfor i in range(len(unique_classes)):\n    for j in range(i + 1, len(unique_classes)):\n        class1 = unique_classes[i]\n        class2 = unique_classes[j]\n        \n        # Calculate cosine similarity between embeddings of class1 and class2\n        distances = 1 - cosine_similarity(class_embeddings[class1], class_embeddings[class2])\n        \n        # Calculate the average distance for this pair of classes\n        avg_distance = np.mean(distances)\n        \n        # Store the average distance in the dictionary\n        inter_class_distances[(class1, class2)] = avg_distance\n\n# Convert inter_class_distances into a distance matrix\nn_classes = len(unique_classes)\ndistance_matrix = np.zeros((n_classes, n_classes))\n\nfor i, class1 in enumerate(unique_classes):\n    for j, class2 in enumerate(unique_classes):\n        if i < j:\n            distance_matrix[i, j] = inter_class_distances[(class1, class2)]\n        elif i > j:\n            distance_matrix[i, j] = inter_class_distances[(class2, class1)]\n\n# Convert the distance_matrix to its condensed form\ncondensed_distance_matrix = pdist(distance_matrix)\n\n# Perform hierarchical clustering\nlinkage_matrix = linkage(condensed_distance_matrix, method='average')\n\n# Create a dendrogram\nplt.figure(figsize=(12, 8))\ndendrogram(linkage_matrix, labels=unique_classes, orientation='top', leaf_font_size=10)\nplt.title(\"Hierarchical Clustering Dendrogram\")\nplt.xlabel(\"Classes\")\nplt.ylabel(\"Distance\")\nplt.show()\n","pos":11,"state":"done","type":"cell"}
{"cell_type":"code","id":"e6d6ea","input":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n# Specify the perplexity and learning rate values\nperplexity_value = 30  # You can adjust this value\nlearning_rate_value = 30  # You can adjust this value\nfilename = f'tsne_visualization_bertbase_perplexity{perplexity_value}_learning_rate{learning_rate_value}.png'\n# Initialize the t-SNE model with specified perplexity and learning rate\ntsne = TSNE(n_components=2, perplexity=perplexity_value, learning_rate=learning_rate_value, random_state=42)\n\n# Fit the t-SNE model to your data\nembeddings_2d = tsne.fit_transform(embedd_array)\n\n# Convert embeddings_2d array into a DataFrame with index values\ndf = pd.DataFrame(embeddings_2d, columns=['Dimension 1', 'Dimension 2'])\ndf['document'] = documents\n\n# Define colors and markers based on unique classes in the 'document' column\nunique_classes = np.sort(df['document'].unique())  # Sort unique classes alphabetically\nn_classes = len(unique_classes)\ncolors = sns.color_palette('Set1', n_colors=n_classes)  # Custom color palette\nmarkers = 'o'  # Circle markers for all classes\n\n\n\n# Create a scatter plot of the t-SNE embeddings for all classes\nplt.figure(figsize=(10, 8))\n# Set Seaborn style with gridlines\nplt.figure(figsize=(10, 8))\nsns.set(style='white')\nhandles = []\nlabels = []\nfor i, class_name in enumerate(unique_classes):\n    df_class = df[df['document'] == class_name]\n    handle = sns.scatterplot(data=df_class, x='Dimension 1', y='Dimension 2', hue='document', palette=[colors[i]],\n                             marker=markers, edgecolor='k', s=75)\n    handles.append(handle)\n    labels.append(class_name)\n\n# Create a legend for the classes\nax = plt.gca()\nhandles, labels = ax.get_legend_handles_labels()\nlegend = ax.legend(handles, labels, title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xlabel(filename, fontsize=12)\nax.set_ylabel('')\n\n# Remove the title at the top\nplt.title(\"\")  # An empty string removes the title\n#plt.title(\"t-SNE Visualization of Embeddings with Document Class Labels\", fontsize=14)\n#plt.xlabel(\"t-SNE Dimension 1\", fontsize=12)\n#plt.ylabel(\"t-SNE Dimension 2\", fontsize=12)\nplt.xticks([])\nplt.yticks([])\nplt.grid(False)  # Turn off gridlines\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\n\n\n\n\n# Save the plot as a PNG image with perplexity and learning rate in the title\n#filename = f'tsne_visualization_bertbase_perplexity{perplexity_value}_learning_rate{learning_rate_value}.png'\nplt.savefig(filename, dpi=500, bbox_inches='tight')\n\nplt.show()\n","pos":19,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a78099","input":"The following cell was generated by ChatGPT using the prompt:\n\n> can the output from the above block be shown in a network graph?\n\n","pos":13,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"d6b700","input":"The following cell was generated by ChatGPT using the prompt:\n\n> is there a visual way to show the output of the above cell?\n\n ","pos":10,"state":"done","type":"cell"}
{"end":1698295811159,"exec_count":8,"id":"ab29d8","input":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Load a pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize and encode the text data\nencoded_requirements = [tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=128, padding='max_length', return_tensors='pt') for text in requirements]\n\n# Generate BERT embeddings\nbert_embeddings = [bert_model(tokens)['last_hidden_state'].mean(dim=1).squeeze().detach().numpy() for tokens in encoded_requirements]\n\n# Initialize the t-SNE model with specified perplexity and learning rate\nperplexity_value = 50\nlearning_rate_value = 10\ntsne = TSNE(n_components=2, perplexity=perplexity_value, learning_rate=learning_rate_value, random_state=42)\n\n# Fit the t-SNE model to the BERT embeddings\nembeddings_2d = tsne.fit_transform(bert_embeddings)\n\n# Convert labels to binary values (0 for 'requirement', 1 for 'standard')\nbinary_labels = [0 if label == 'requirement' else 1 for label in labels]\n\n# Define colors and markers for the two classes\ncolors = sns.color_palette('Set1', n_colors=2)\nmarkers = 'o'\n\n# Create a DataFrame with the t-SNE embeddings and binary labels\ndf_tsne = pd.DataFrame(embeddings_2d, columns=['Dimension 1', 'Dimension 2'])\ndf_tsne['label'] = binary_labels\n\n# Create a scatter plot of the t-SNE embeddings for both classes\nplt.figure(figsize=(10, 8))\nsns.set(style='whitegrid')\n\nfor label in set(binary_labels):\n    df_label = df_tsne[df_tsne['label'] == label]\n    sns.scatterplot(data=df_label, x='Dimension 1', y='Dimension 2', hue='label', palette=[colors[label]], markers=markers, edgecolor='k', s=100)\n\n# Create legends for the classes\nclass_labels = ['requirement', 'standard']\nlegend_handles = [plt.Line2D([0], [0], marker=markers, color='w', label=class_labels[label], markersize=10, markerfacecolor=colors[label]) for label in set(binary_labels)]\nplt.legend(handles=legend_handles, title='Classes')\n\nplt.title(\"t-SNE Visualization of BERT Embeddings with Class Labels\", fontsize=16)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Save the plot as a PNG image\nplt.savefig('tsne_visualization.png', dpi=300, bbox_inches='tight')\n\nplt.show()\n\n","kernel":"python3-ubuntu","output":{"0":{"name":"stderr","text":"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"},"1":{"ename":"AttributeError","evalue":"'list' object has no attribute 'shape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39mperplexity_value, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate_value, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Fit the t-SNE model to the BERT embeddings\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m embeddings_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Convert labels to binary values (0 for 'requirement', 1 for 'standard')\u001b[39;00m\n\u001b[1;32m     27\u001b[0m binary_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequirement\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py:1118\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m-> 1118\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py:828\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    829\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"]}},"pos":6,"start":1698295735461,"state":"done","type":"cell"}
{"end":1698296205771,"exec_count":6,"id":"7d8980","input":"from transformers import BertTokenizer, BertModel\nimport torch\n\n# Load the BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize and encode the text data\nencoded_text = [tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\") for text in requirements]\n\n# Obtain BERT embeddings for each text\nembeddings = [model(**text).last_hidden_state.mean(dim=1).squeeze().detach().numpy() for text in encoded_text]\n","kernel":"python3-ubuntu","pos":3.5,"start":1698296153715,"state":"done","type":"cell"}
{"end":1698297416400,"exec_count":15,"id":"6a793f","input":"from sklearn.manifold import TSNE\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n\n# Create a DataFrame with the t-SNE embeddings and labels\ndf = pd.DataFrame(embeddings_2d, columns=['Dimension 1', 'Dimension 2'])\ndf['label'] = labels\ndf['document_class'] = df['document']  # Create a new column for document classes\n\n# Create a dictionary to map document classes to unique colors\nunique_document_classes = df['document_class'].unique()\ncolors = sns.color_palette('husl', n_colors=len(unique_document_classes))\ndocument_class_color_mapping = {doc_class: color for doc_class, color in zip(unique_document_classes, colors)}\n\n# Assign a color to each data point based on its document class\ndf['color'] = df['document_class'].map(document_class_color_mapping)\n\n# Create a scatter plot of the t-SNE embeddings for different document classes\nplt.figure(figsize=(10, 8))\nsns.set(style='whitegrid')  # Set Seaborn style with gridlines\n\nfor doc_class, color in document_class_color_mapping.items():\n    df_class = df[df['document_class'] == doc_class]\n    marker = 's' if doc_class == 'Triangle' else 'o'\n    sns.scatterplot(data=df_class, x='Dimension 1', y='Dimension 2', hue='document_class', palette=[color], marker=marker, edgecolor='k', s=100)\n    \n\n\n# Create legends for the document classes\nlegend_handles = [plt.Line2D([0], [0], marker=marker, color='w', label=doc_class, markersize=10,\n                              markerfacecolor=color) for doc_class, color in document_class_color_mapping.items()]\nplt.legend(handles=legend_handles, title='Document Classes')\n\nplt.title(\"t-SNE Visualization of Embeddings with Document Classes and IDs\", fontsize=16)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)  # Turn off gridlines\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Save the plot as a PNG image\nplt.savefig('tsne_visualization_with_classes.png', dpi=300, bbox_inches='tight')\n\nplt.show()\n\n","kernel":"python3-ubuntu","output":{"0":{"ename":"KeyError","evalue":"'document'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'document'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(embeddings_2d, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDimension 1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDimension 2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m---> 12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument_class\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Create a new column for document classes\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create a dictionary to map document classes to unique colors\u001b[39;00m\n\u001b[1;32m     15\u001b[0m unique_document_classes \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument_class\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 'document'"]}},"pos":3.875,"start":1698297377271,"state":"done","type":"cell"}
{"id":0,"time":1698289435697,"type":"user"}
{"last_load":1698289434816,"type":"file"}