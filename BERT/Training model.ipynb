{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset  \n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchvision.transforms as transforms\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification , Trainer , TrainingArguments\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_metric\n",
    "from transformers import Trainer\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"embed_data\")\n",
    "df = df.loc[(df.label == \"standard\") | (df.label == \"requirement\")]\n",
    "print(df.groupby(\"label\").count())\n",
    "print(df.head())\n",
    "\n",
    "text = df.text.tolist()\n",
    "labels = df.label.map({'standard': 1, 'requirement': 0}).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "requirements = df.text.tolist()\n",
    "labels = df.label.map({'standard': 1, 'requirement': 0}).tolist()\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_requs , test_requs , train_labels , test_labels = train_test_split (\n",
    "requirements , labels , random_state =500 , test_size =.2)\n",
    "\n",
    "train_requs , val_requs , train_labels , val_labels = train_test_split (\n",
    "train_requs , train_labels , random_state =501 , test_size =.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer ( train_requs , truncation = True , padding = True )\n",
    "val_encodings = tokenizer ( val_requs , truncation = True , padding = True )\n",
    "test_encodings = tokenizer ( test_requs , truncation = True , padding = True )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RequDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RequDataset(train_encodings, train_labels)\n",
    "val_dataset = RequDataset(val_encodings, val_labels)\n",
    "test_dataset = RequDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, AdamW, BertTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback, get_linear_schedule_with_warmup\n",
    "from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create collapsible output widget\n",
    "output = widgets.Output()\n",
    "output_collapsible = widgets.Accordion([output])\n",
    "output_collapsible.set_title(0, 'Model Output')\n",
    "display(output_collapsible)\n",
    "\n",
    "# Function to display content in the output widget\n",
    "def display_in_output(content):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        display(content)\n",
    "\n",
    "\n",
    "\n",
    "# Define your training arguments first\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Define your model and optimizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n",
    "\n",
    "# Load the metric\n",
    "metric = load_metric(\"matthews_correlation\")\n",
    "\n",
    "# Define your compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels_string = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels_string)\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Define your early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.01)\n",
    "\n",
    "# Manually calculate the total number of training steps\n",
    "total_train_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "\n",
    "# Define your learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(total_train_steps * training_args.warmup_ratio),\n",
    "    num_training_steps=total_train_steps\n",
    ")\n",
    "\n",
    "# Initialize lists to store training and evaluation metrics\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "eval_matthews_corrs = []\n",
    "\n",
    "# Add the early stopping callback to the trainer\n",
    "trainer.add_callback(early_stopping)\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    # Training\n",
    "    trainer.train()\n",
    "\n",
    "    # Validation\n",
    "    trainer.evaluate()\n",
    "\n",
    "    # Create a collapsible widget for model output\n",
    "    model_output = widgets.Output()\n",
    "    model_output_collapsible = widgets.Accordion([model_output])\n",
    "    model_output_collapsible.set_title(0, f'Epoch {epoch + 1}/{training_args.num_train_epochs}')\n",
    "    \n",
    "    # Function to display content in the model_output widget\n",
    "    def display_in_model_output(content):\n",
    "        with model_output:\n",
    "            clear_output(wait=True)\n",
    "            display(content)\n",
    "    \n",
    "    # Get training and evaluation metrics\n",
    "    train_loss = trainer.callback_metrics['train_loss']\n",
    "    eval_loss = trainer.callback_metrics['eval_loss']\n",
    "    eval_matthews_corr = trainer.callback_metrics['eval_matthews_correlation']\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    train_losses.append(train_loss)\n",
    "    eval_losses.append(eval_loss)\n",
    "    eval_matthews_corrs.append(eval_matthews_corr)\n",
    "    \n",
    "    # Display training and validation metrics in model output\n",
    "    display_in_model_output(f\"Training Loss: {train_loss:.4f}\")\n",
    "    display_in_model_output(f\"Validation Loss: {eval_loss:.4f}\")\n",
    "    display_in_model_output(f\"Matthews Correlation: {eval_matthews_corr:.4f}\")\n",
    "    \n",
    "    # Display the collapsible model output widget\n",
    "    display(model_output_collapsible)\n",
    "\n",
    "    # Check for early stopping\n",
    "    if early_stopping.early_stopping.should_stop:\n",
    "        display_in_output(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    # Update the learning rate using the scheduler\n",
    "    for _ in range(total_train_steps):\n",
    "        scheduler.step()\n",
    "\n",
    "# Access training and evaluation metrics from the lists\n",
    "final_train_loss = train_losses[-1]\n",
    "final_eval_loss = eval_losses[-1]\n",
    "final_eval_matthews_corr = eval_matthews_corrs[-1]\n",
    "\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_eval_loss:.4f}\")\n",
    "print(f\"Final Matthews Correlation: {final_eval_matthews_corr:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn . metrics import matthews_corrcoef as mcc\n",
    "mcc_test_labels = [ label for label in test_labels ]\n",
    "mcc_preds = [ label for label in preds . tolist () ]\n",
    "print ( mcc_preds )\n",
    "print ( mcc_test_labels )\n",
    "print ( mcc ( mcc_test_labels , mcc_preds ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(test_labels, preds))\n",
    "print(metrics.classification_report(test_labels, preds, target_names=['Standard', 'Requirement']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the model's weights in the \"savedmodels\" folder\n",
    "model.save_pretrained('savedmodels')\n",
    "\n",
    "# Optionally, save the model's configuration and tokenizer in the same folder\n",
    "model.config.save_pretrained('savedmodels')\n",
    "tokenizer.save_pretrained('savedmodels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
